
@inproceedings{parkhurst_single_2006,
	title = {From {Single} {Core} to {Multi}-{Core}: {Preparing} for a new exponential},
	shorttitle = {From {Single} {Core} to {Multi}-{Core}},
	doi = {10.1109/ICCAD.2006.320067},
	abstract = {In the past, processor design trends were dominated by increasingly complex feature sets, higher clock speeds, growing thermal envelopes and increasing power dissipation. Recently, clock speeds have tapered and thermal and power dissipation envelopes have remained flat. However, the demand for increasing performance continues which has fueled the move to integrated multiple processor (multi-core) designs. This paper discusses this trend towards multi-core processor designs, the design challenges that accompany it and a view of the research required to support it},
	booktitle = {2006 {IEEE}/{ACM} {International} {Conference} on {Computer} {Aided} {Design}},
	author = {Parkhurst, Jeff and Darringer, John and Grundmann, Bill},
	month = nov,
	year = {2006},
	note = {ISSN: 1558-2434},
	keywords = {Clocks, Frequency, Gate leakage, Microprocessors, Moore's Law, Multicore processing, Permission, Power dissipation, Process design, Threshold voltage},
	pages = {67--72},
	file = {IEEE Xplore Abstract Record:/Users/neilweidinger/Zotero/storage/3B2JNI3T/4110155.html:text/html;Full Text:/Users/neilweidinger/Zotero/storage/JFR9BAR5/Parkhurst et al. - 2006 - From Single Core to Multi-Core Preparing for a ne.pdf:application/pdf},
}

@inproceedings{gepner_multi-core_2006,
	title = {Multi-{Core} {Processors}: {New} {Way} to {Achieve} {High} {System} {Performance}},
	shorttitle = {Multi-{Core} {Processors}},
	doi = {10.1109/PARELEC.2006.54},
	abstract = {Multi-core processors represent an evolutionary change in conventional computing as well setting the new trend for high performance computing (HPC) - but parallelism is nothing new. Intel has a long history with the concept of parallelism and the development of hardware-enhanced threading capabilities. Intel has been delivering threading-capable products for more than a decade. The move toward chip-level multiprocessing architectures with a large number of cores continues to offer dramatically increased performance and power characteristics. Nonetheless, this move also presents significant challenges. This paper describes how far the industry has progressed and evaluates some of the challenges we are facing with multi-core processors and some of the solutions that have been developed},
	booktitle = {International {Symposium} on {Parallel} {Computing} in {Electrical} {Engineering} ({PARELEC}'06)},
	author = {Gepner, P. and Kowalik, M.F.},
	month = sep,
	year = {2006},
	keywords = {Clocks, Frequency, Multicore processing, Computer architecture, High performance computing, History, Microarchitecture, Parallel processing, Silicon, System performance},
	pages = {9--13},
	file = {IEEE Xplore Full Text PDF:/Users/neilweidinger/Zotero/storage/DNN82CKG/Gepner and Kowalik - 2006 - Multi-Core Processors New Way to Achieve High Sys.pdf:application/pdf},
}

@article{hennessy_new_2019,
	title = {A new golden age for computer architecture},
	volume = {62},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3282307},
	doi = {10.1145/3282307},
	abstract = {Innovations like domain-specific hardware, enhanced security, open instruction sets, and agile chip development will lead the way.},
	number = {2},
	urldate = {2021-10-24},
	journal = {Communications of the ACM},
	author = {Hennessy, John L. and Patterson, David A.},
	month = jan,
	year = {2019},
	pages = {48--60},
	file = {Full Text PDF:/Users/neilweidinger/Zotero/storage/LBHZTUSP/Hennessy and Patterson - 2019 - A new golden age for computer architecture.pdf:application/pdf},
}

@article{conte_rebooting_2017,
	title = {Rebooting {Computing}: {The} {Road} {Ahead}},
	volume = {50},
	issn = {0018-9162},
	shorttitle = {Rebooting {Computing}},
	url = {http://ieeexplore.ieee.org/document/7807179/},
	doi = {10.1109/MC.2017.8},
	number = {1},
	urldate = {2021-10-24},
	journal = {Computer},
	author = {Conte, Thomas M. and DeBenedictis, Erik P. and Gargini, Paolo A. and Track, Elie},
	month = jan,
	year = {2017},
	pages = {20--29},
	file = {Conte et al. - 2017 - Rebooting Computing The Road Ahead.pdf:/Users/neilweidinger/Zotero/storage/PC8KZ5JL/Conte et al. - 2017 - Rebooting Computing The Road Ahead.pdf:application/pdf},
}

@article{venu_multi-core_2011,
	title = {Multi-core processors - {An} overview},
	url = {http://arxiv.org/abs/1110.3535},
	abstract = {Microprocessors have revolutionized the world we live in and continuous efforts are being made to manufacture not only faster chips but also smarter ones. A number of techniques such as data level parallelism, instruction level parallelism and hyper threading (Intel's HT) already exists which have dramatically improved the performance of microprocessor cores. This paper briefs on evolution of multi-core processors followed by introducing the technology and its advantages in today's world. The paper concludes by detailing on the challenges currently faced by multi-core processors and how the industry is trying to address these issues.},
	urldate = {2021-10-24},
	journal = {arXiv:1110.3535 [cs]},
	author = {Venu, Balaji},
	month = oct,
	year = {2011},
	note = {arXiv: 1110.3535},
	keywords = {Computer Science - Hardware Architecture},
	annote = {Comment: 6 pages, Best Literature review},
	file = {arXiv Fulltext PDF:/Users/neilweidinger/Zotero/storage/477ZBKFI/Venu - 2011 - Multi-core processors - An overview.pdf:application/pdf;arXiv.org Snapshot:/Users/neilweidinger/Zotero/storage/EL72J83J/1110.html:text/html},
}

@article{borkar_future_2011,
	title = {The future of microprocessors},
	volume = {54},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/1941487.1941507},
	doi = {10.1145/1941487.1941507},
	abstract = {Energy efficiency is the new fundamental limiter of processor performance, way beyond numbers of processors.},
	number = {5},
	urldate = {2021-10-27},
	journal = {Communications of the ACM},
	author = {Borkar, Shekhar and Chien, Andrew A.},
	month = may,
	year = {2011},
	pages = {67--77},
	file = {Full Text PDF:/Users/neilweidinger/Zotero/storage/TYF4E37D/Borkar and Chien - 2011 - The future of microprocessors.pdf:application/pdf},
}

@book{patterson_computer_2021,
	address = {Cambridge, MA},
	edition = {Second edition},
	series = {{RISC}-{V} edition},
	title = {Computer {Organization} and {Design}: {The} {Hardware} {Software} {Interface}},
	isbn = {978-0-12-820331-6},
	shorttitle = {Computer organization and design},
	language = {eng},
	publisher = {Elsevier},
	author = {Patterson, David A. and Hennessy, John L.},
	year = {2021},
}

@article{etiemble_45-year_2018,
	title = {45-year {CPU} evolution: one law and two equations},
	shorttitle = {45-year {CPU} evolution},
	url = {http://arxiv.org/abs/1803.00254},
	abstract = {Moore's law and two equations allow to explain the main trends of CPU evolution since MOS technologies have been used to implement microprocessors.},
	urldate = {2021-10-28},
	journal = {arXiv:1803.00254 [cs]},
	author = {Etiemble, Daniel},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.00254},
	keywords = {Computer Science - Hardware Architecture},
	file = {arXiv Fulltext PDF:/Users/neilweidinger/Zotero/storage/M5GQ3P7X/Etiemble - 2018 - 45-year CPU evolution one law and two equations.pdf:application/pdf;arXiv.org Snapshot:/Users/neilweidinger/Zotero/storage/5MX7EYNY/1803.html:text/html},
}

@misc{noauthor_free_nodate,
	title = {The {Free} {Lunch} {Is} {Over}: {A} {Fundamental} {Turn} {Toward} {Concurrency} in {Software}},
	url = {http://www.gotw.ca/publications/concurrency-ddj.htm},
	urldate = {2021-10-30},
	file = {The Free Lunch Is Over\: A Fundamental Turn Toward Concurrency in Software:/Users/neilweidinger/Zotero/storage/IM56D2HK/concurrency-ddj.html:text/html},
}

@book{bryant_computer_2016,
	address = {Boston},
	edition = {Third edition},
	title = {Computer systems: a programmer's perspective},
	isbn = {978-0-13-409266-9},
	shorttitle = {Computer systems},
	publisher = {Pearson},
	author = {Bryant, Randal E. and O'Hallaron, David R.},
	year = {2016},
	keywords = {Computer systems, Computers, Telecommunication, User interfaces (Computer systems)},
}

@article{patterson_trouble_2010,
	title = {The trouble with multi-core},
	volume = {47},
	issn = {0018-9235},
	url = {http://ieeexplore.ieee.org/document/5491011/},
	doi = {10.1109/MSPEC.2010.5491011},
	language = {en},
	number = {7},
	urldate = {2021-11-02},
	journal = {IEEE Spectrum},
	author = {Patterson, David},
	month = jul,
	year = {2010},
	pages = {28--32, 53},
	file = {Patterson - 2010 - The trouble with multi-core.pdf:/Users/neilweidinger/Zotero/storage/T8XKUA7V/Patterson - 2010 - The trouble with multi-core.pdf:application/pdf},
}

@article{creeger_multicore_2005,
	title = {Multicore {CPUs} for the {Masses}: {Will} increased {CPU} bandwidth translate into usable desktop performance?},
	volume = {3},
	issn = {1542-7730},
	shorttitle = {Multicore {CPUs} for the {Masses}},
	url = {https://doi.org/10.1145/1095408.1095423},
	doi = {10.1145/1095408.1095423},
	abstract = {Multicore is the new hot topic in the latest round of CPUs from Intel, AMD, Sun, etc. With clock speed increases becoming more and more difficult to achieve, vendors have turned to multicore CPUs as the best way to gain additional performance. Customers are excited about the promise of more performance through parallel processors for the same real estate investment.},
	number = {7},
	urldate = {2021-11-04},
	journal = {Queue},
	author = {Creeger, Mache},
	month = sep,
	year = {2005},
	pages = {64--ff},
	file = {Full Text PDF:/Users/neilweidinger/Zotero/storage/9VXPWJF7/Creeger - 2005 - Multicore CPUs for the Masses Will increased CPU .pdf:application/pdf},
}

@phdthesis{blumofe_executing_1995,
	type = {Thesis},
	title = {Executing multithreaded programs efficiently},
	copyright = {M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided URL for inquiries about permission.},
	url = {https://dspace.mit.edu/handle/1721.1/11095},
	abstract = {Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1995.},
	language = {eng},
	urldate = {2021-11-05},
	school = {Massachusetts Institute of Technology},
	author = {Blumofe, Robert D. (Robert David)},
	year = {1995},
	note = {Accepted: 2005-08-17T23:21:41Z},
	file = {Snapshot:/Users/neilweidinger/Zotero/storage/647RE3A6/11095.html:text/html;Blumofe - 1995 - Executing multithreaded programs efficiently.pdf:/Users/neilweidinger/Zotero/storage/FFTTVYWR/Blumofe - 1995 - Executing multithreaded programs efficiently.pdf:application/pdf},
}

@article{lee_problem_2006,
	title = {The problem with threads},
	volume = {39},
	issn = {1558-0814},
	doi = {10.1109/MC.2006.180},
	abstract = {For concurrent programming to become mainstream, we must discard threads as a programming model. Nondeterminism should be judiciously and carefully introduced where needed, and it should be explicit in programs. In general-purpose software engineering practice, we have reached a point where one approach to concurrent programming dominates all others namely, threads, sequential processes that share memory. They represent a key concurrency model supported by modern computers, programming languages, and operating systems. In scientific computing, where performance requirements have long demanded concurrent programming, data-parallel language extensions and message-passing libraries such as PVM, MPI, and OpenMP dominate over threads for concurrent programming. Computer architectures intended for scientific computing often differ significantly from so-called general-purpose architectures.},
	number = {5},
	journal = {Computer},
	author = {Lee, E.A.},
	month = may,
	year = {2006},
	note = {Conference Name: Computer},
	keywords = {Coordination languages, Design patterns, Nondeterminism, Programming paradigms, Threads, Computational modeling, Costs, Distributed computing, Embedded computing, Embedded software, Equations, Object oriented modeling},
	pages = {33--42},
	file = {IEEE Xplore Full Text PDF:/Users/neilweidinger/Zotero/storage/JJ564ZRH/Lee - 2006 - The problem with threads.pdf:application/pdf},
}

@book{cormen_introduction_2009,
	address = {Cambridge, Mass},
	edition = {3rd ed},
	title = {Introduction to algorithms},
	isbn = {978-0-262-03384-8 978-0-262-53305-8},
	publisher = {MIT Press},
	editor = {Cormen, Thomas H.},
	year = {2009},
	note = {OCLC: ocn311310321},
	keywords = {Computer algorithms, Computer programming},
}

@book{herlihy_art_2012,
	address = {Amsterdam},
	edition = {Revised first edition},
	title = {The art of multiprocessor programming},
	isbn = {978-0-12-397337-5},
	publisher = {Morgan Kaufmann},
	author = {Herlihy, Maurice and Shavit, Nir},
	year = {2012},
	keywords = {Multiprocessors, Multiprogramming (Electronic computers)},
}

@inproceedings{singer_proactive_2019,
	address = {New York, NY, USA},
	series = {{PPoPP} '19},
	title = {Proactive work stealing for futures},
	isbn = {978-1-4503-6225-2},
	url = {https://doi.org/10.1145/3293883.3295735},
	doi = {10.1145/3293883.3295735},
	abstract = {The use of futures provides a flexible way to express parallelism and can generate arbitrary dependences among parallel subcomputations. The additional flexibility that futures provide comes with a cost, however. When scheduled using classic work stealing, a program with futures, compared to a program that uses only fork-join parallelism, can incur a much higher number of "deviations," a metric for evaluating the performance of parallel executions. All prior works assume a parsimonious work-stealing scheduler, however, where a worker thread (surrogate of a processor) steals work only when its local deque becomes empty. In this work, we investigate an alternative scheduling approach, called ProWS, where the workers perform proactive work stealing when handling future operations. We show that ProWS, for programs that use futures, can provide provably efficient execution time and equal or better bounds on the number of deviations compared to classic parsimonious work stealing. Given a computation with T1 work and T∞ span, ProWS executes the computation on P processors in expected time O(T1/P + T∞ lg P), with an additional lg P overhead on the span term compared to the parsimonious variant. For structured use of futures, where each future is single touch with no race on the future handle, the algorithm incurs deviations, matching that of the parsimonious variant. For general use of futures, the algorithm incurs O(mkT∞ + PT∞ lg P) deviations, where mk is the maximum number of future touches that are logically parallel. Compared to the bound for the parsimonious variant, O(kT∞ + PT∞), with k being the total number of touches in the entire computation, this bound is better assuming mk = Ω(P lg P) and is smaller than k, which holds true for all the benchmarks we examined.},
	urldate = {2022-03-29},
	booktitle = {Proceedings of the 24th {Symposium} on {Principles} and {Practice} of {Parallel} {Programming}},
	publisher = {Association for Computing Machinery},
	author = {Singer, Kyle and Xu, Yifan and Lee, I-Ting Angelina},
	month = feb,
	year = {2019},
	pages = {257--271},
	file = {Full Text PDF:/Users/neilweidinger/Zotero/storage/FRATP4B7/Singer et al. - 2019 - Proactive work stealing for futures.pdf:application/pdf},
}

@inproceedings{muller_latency-hiding_2016,
	address = {New York, NY, USA},
	series = {{SPAA} '16},
	title = {Latency-{Hiding} {Work} {Stealing}: {Scheduling} {Interacting} {Parallel} {Computations} with {Work} {Stealing}},
	isbn = {978-1-4503-4210-0},
	shorttitle = {Latency-{Hiding} {Work} {Stealing}},
	url = {https://doi.org/10.1145/2935764.2935793},
	doi = {10.1145/2935764.2935793},
	abstract = {With the rise of multicore computers, parallel applications no longer consist solely of computational, batch workloads, but also include applications that may, for example, take input from a user, access secondary storage or the network, or perform remote procedure calls. Such operations can incur substantial latency, requiring the program to wait for a response. In the current state of the art, the theoretical models of parallelism and parallel scheduling algorithms do not account for latency. In this work, we extend the dag (Directed Acyclic Graph) model for parallelism to account for latency and present a work-stealing algorithm that hides latency to improve performance. This algorithm allows user-level threads to suspend without blocking the underlying worker, usually a system thread. When a user-level thread suspends, the algorithm switches to another thread. Using extensions of existing techniques as well as new technical devices, we bound the running time of our scheduler on a parallel computation. We also briefly present a prototype implementation of the algorithm and some preliminary empirical findings.},
	urldate = {2022-03-29},
	booktitle = {Proceedings of the 28th {ACM} {Symposium} on {Parallelism} in {Algorithms} and {Architectures}},
	publisher = {Association for Computing Machinery},
	author = {Muller, Stefan K. and Acar, Umut A.},
	month = jul,
	year = {2016},
	keywords = {latency, latency hiding, parallel computing, work stealing},
	pages = {71--82},
	file = {Full Text PDF:/Users/neilweidinger/Zotero/storage/2USCYDI3/Muller and Acar - 2016 - Latency-Hiding Work Stealing Scheduling Interacti.pdf:application/pdf},
}

@inproceedings{conway_multiprocessor_1963,
	address = {New York, NY, USA},
	series = {{AFIPS} '63 ({Fall})},
	title = {A multiprocessor system design},
	isbn = {978-1-4503-7883-3},
	url = {https://doi.org/10.1145/1463822.1463838},
	doi = {10.1145/1463822.1463838},
	abstract = {Parallel processing is not so mysterious a concept as the dearth of algorithms which explicitly use it might suggest. As a rule of thumb, if N processes are performed and the outcome is independent of the order in which their steps are executed, provided that within each process the order of steps is preserved, then any or all of the processes can be performed simultaneously, if conflicts arising from multiple access to common storage can be resolved. All the elements of a matrix sum may be evaluated in parallel. The ith summand of all elements of a matrix product may be computed simultaneously. In an internal merge sort all strings in any pass may be created at the same time. All the coroutines of a separable program may be run concurrently.},
	urldate = {2022-03-30},
	booktitle = {Proceedings of the {November} 12-14, 1963, fall joint computer conference},
	publisher = {Association for Computing Machinery},
	author = {Conway, Melvin E.},
	month = nov,
	year = {1963},
	pages = {139--146},
}

@article{nyman_notes_2016,
	title = {Notes on the {History} of {Fork} and {Join}},
	volume = {38},
	issn = {1934-1547},
	doi = {10.1109/MAHC.2016.34},
	abstract = {The fork call allows a process (or running program) to create new processes. On multiprocessor systems, these processes can run concurrently in parallel. Since its birth 50 years ago, the fork has remained a central element of modern computing, both with regard to software development principles and, by extension, to hardware design, which increasingly accommodates parallelism in process execution. This article looks back at the birth of the fork system call to share, as remembered by its pioneers.},
	number = {3},
	journal = {IEEE Annals of the History of Computing},
	author = {Nyman, Linus and Laakso, Mikael},
	month = jul,
	year = {2016},
	note = {Conference Name: IEEE Annals of the History of Computing},
	keywords = {fork and join, fork system call, history of computing, Melvin Conway, Project Genie},
	pages = {84--87},
	file = {IEEE Xplore Full Text PDF:/Users/neilweidinger/Zotero/storage/DB8GBZSF/Nyman and Laakso - 2016 - Notes on the History of Fork and Join.pdf:application/pdf},
}

@inproceedings{frigo_implementation_1998,
	address = {New York, NY, USA},
	series = {{PLDI} '98},
	title = {The implementation of the {Cilk}-5 multithreaded language},
	isbn = {978-0-89791-987-6},
	url = {https://doi.org/10.1145/277650.277725},
	doi = {10.1145/277650.277725},
	abstract = {The fifth release of the multithreaded language Cilk uses a provably good "work-stealing" scheduling algorithm similar to the first system, but the language has been completely redesigned and the runtime system completely reengineered. The efficiency of the new implementation was aided by a clear strategy that arose from a theoretical analysis of the scheduling algorithm: concentrate on minimizing overheads that contribute to the work, even at the expense of overheads that contribute to the critical path. Although it may seem counterintuitive to move overheads onto the critical path, this "work-first" principle has led to a portable Cilk-5 implementation in which the typical cost of spawning a parallel thread is only between 2 and 6 times the cost of a C function call on a variety of contemporary machines. Many Cilk programs run on one processor with virtually no degradation compared to equivalent C programs. This paper describes how the work-first principle was exploited in the design of Cilk-5's compiler and its runtime system. In particular, we present Cilk-5's novel "two-clone" compilation strategy and its Dijkstra-like mutual-exclusion protocol for implementing the ready deque in the work-stealing scheduler.},
	urldate = {2022-03-30},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} 1998 conference on {Programming} language design and implementation},
	publisher = {Association for Computing Machinery},
	author = {Frigo, Matteo and Leiserson, Charles E. and Randall, Keith H.},
	month = may,
	year = {1998},
	keywords = {critical path, multithreading, parallel computing, programming language, runtime system, work},
	pages = {212--223},
	file = {Submitted Version:/Users/neilweidinger/Zotero/storage/2VBRLKS7/Frigo et al. - 1998 - The implementation of the Cilk-5 multithreaded lan.pdf:application/pdf},
}

@misc{noauthor_advanced_nodate,
	title = {Advanced {HPC} {Threading}: {Intel}® {oneAPI} {Threading} {Building} {Blocks}},
	shorttitle = {Advanced {HPC} {Threading}},
	url = {https://www.intel.com/content/www/us/en/developer/tools/oneapi/onetbb.html},
	abstract = {Simplify efforts to add parallelism to an application via a flexible runtime library that automatically maps logical parallelism onto threads.},
	language = {en},
	urldate = {2022-03-30},
	journal = {Intel},
	file = {Snapshot:/Users/neilweidinger/Zotero/storage/IE9GCXCM/onetbb.html:text/html},
}

@inproceedings{lea_java_2000,
	address = {New York, NY, USA},
	series = {{JAVA} '00},
	title = {A {Java} fork/join framework},
	isbn = {978-1-58113-288-5},
	url = {https://doi.org/10.1145/337449.337465},
	doi = {10.1145/337449.337465},
	urldate = {2022-03-30},
	booktitle = {Proceedings of the {ACM} 2000 conference on {Java} {Grande}},
	publisher = {Association for Computing Machinery},
	author = {Lea, Doug},
	month = jun,
	year = {2000},
	pages = {36--43},
	file = {Submitted Version:/Users/neilweidinger/Zotero/storage/B8VJYHYR/Lea - 2000 - A Java forkjoin framework.pdf:application/pdf},
}

@inproceedings{blumofe_cilk_1995,
	address = {New York, NY, USA},
	series = {{PPOPP} '95},
	title = {Cilk: an efficient multithreaded runtime system},
	isbn = {978-0-89791-700-1},
	shorttitle = {Cilk},
	url = {https://doi.org/10.1145/209936.209958},
	doi = {10.1145/209936.209958},
	abstract = {Cilk (pronounced “silk”) is a C-based runtime system for multi-threaded parallel programming. In this paper, we document the efficiency of the Cilk work-stealing scheduler, both empirically and analytically. We show that on real and synthetic applications, the “work” and “critical path” of a Cilk computation can be used to accurately model performance. Consequently, a Cilk programmer can focus on reducing the work and critical path of his computation, insulated from load balancing and other runtime scheduling issues. We also prove that for the class of “fully strict” (well-structured) programs, the Cilk scheduler achieves space, time and communication bounds all within a constant factor of optimal. The Cilk runtime system currently runs on the Connection Machine CM5 MPP, the Intel Paragon MPP, the Silicon Graphics Power Challenge SMP, and the MIT Phish network of workstations. Applications written in Cilk include protein folding, graphic rendering, backtrack search, and the *Socrates chess program, which won third prize in the 1994 ACM International Computer Chess Championship.},
	urldate = {2022-03-30},
	booktitle = {Proceedings of the fifth {ACM} {SIGPLAN} symposium on {Principles} and practice of parallel programming},
	publisher = {Association for Computing Machinery},
	author = {Blumofe, Robert D. and Joerg, Christopher F. and Kuszmaul, Bradley C. and Leiserson, Charles E. and Randall, Keith H. and Zhou, Yuli},
	month = aug,
	year = {1995},
	pages = {207--216},
}

@article{blumofe_scheduling_1999,
	title = {Scheduling multithreaded computations by work stealing},
	volume = {46},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/324133.324234},
	doi = {10.1145/324133.324234},
	abstract = {This paper studies the problem of efficiently schedulling fully strict (i.e., well-structured) multithreaded computations on parallel computers. A popular and practical method of scheduling this kind of dynamic MIMD-style computation is “work stealing,” in which processors needing work steal computational threads from other processors. In this paper, we give the first provably good work-stealing scheduler for multithreaded computations with dependencies. Specifically, our analysis shows that the expected time to execute a fully strict computation on P processors using our work-stealing scheduler is T1/P + O(T ∞ , where T1 is the minimum serial execution time of the multithreaded computation and (T ∞ is the minimum execution time with an infinite number of processors. Moreover, the space required by the execution is at most S1P, where S1 is the minimum serial space requirement. We also show that the expected total communication of the algorithm is at most O(PT ∞( 1 + nd)Smax), where Smax is the size of the largest activation record of any thread and nd is the maximum number of times that any thread synchronizes with its parent. This communication bound justifies the folk wisdom that work-stealing schedulers are more communication efficient than their work-sharing counterparts. All three of these bounds are existentially optimal to within a constant factor.},
	number = {5},
	urldate = {2022-03-30},
	journal = {Journal of the ACM},
	author = {Blumofe, Robert D. and Leiserson, Charles E.},
	month = sep,
	year = {1999},
	keywords = {critical-path length, multiprocessor, multithreading, randomized algorithm, thread scheduling, work stealing},
	pages = {720--748},
}

@inproceedings{arora_thread_1998,
	address = {New York, NY, USA},
	series = {{SPAA} '98},
	title = {Thread scheduling for multiprogrammed multiprocessors},
	isbn = {978-0-89791-989-0},
	url = {https://doi.org/10.1145/277651.277678},
	doi = {10.1145/277651.277678},
	urldate = {2022-03-30},
	booktitle = {Proceedings of the tenth annual {ACM} symposium on {Parallel} algorithms and architectures},
	publisher = {Association for Computing Machinery},
	author = {Arora, Nimar S. and Blumofe, Robert D. and Plaxton, C. Greg},
	month = jun,
	year = {1998},
	pages = {119--129},
}

@misc{niebler_structured_2020,
	title = {Structured {Concurrency}},
	url = {https://ericniebler.com/2020/11/08/structured-concurrency/},
	abstract = {TL;DR: “Structured concurrency” refers to a way to structure async computations so that child operations are guaranteed to complete before their parents, just the way a function is guaranteed…},
	language = {en-US},
	urldate = {2022-03-30},
	journal = {Eric Niebler},
	author = {Niebler, Eric},
	month = nov,
	year = {2020},
	file = {Snapshot:/Users/neilweidinger/Zotero/storage/TWUPYBIG/structured-concurrency.html:text/html},
}

@misc{smith_notes_nodate,
	title = {Notes on structured concurrency, or: {Go} statement considered harmful — njs blog},
	url = {https://vorpus.org/blog/notes-on-structured-concurrency-or-go-statement-considered-harmful/#what-is-a-go-statement-anyway},
	urldate = {2022-03-30},
	author = {Smith, Nathaniel J.},
	file = {Notes on structured concurrency, or\: Go statement considered harmful — njs blog:/Users/neilweidinger/Zotero/storage/BLJDBMVW/notes-on-structured-concurrency-or-go-statement-considered-harmful.html:text/html},
}

@inproceedings{halstead_implementation_1984,
	address = {New York, NY, USA},
	series = {{LFP} '84},
	title = {Implementation of multilisp: {Lisp} on a multiprocessor},
	isbn = {978-0-89791-142-9},
	shorttitle = {Implementation of multilisp},
	url = {https://doi.org/10.1145/800055.802017},
	doi = {10.1145/800055.802017},
	abstract = {Multilisp is an extension of Lisp (more specifically, of the Lisp dialect Scheme [15]) with additional operators and additional semantics to deal with parallel execution. It is being implemented on the 32-processor Concert multiprocessor. The current implementation is complete enough to run the Multilisp compiler itself, and has been run on Concert prototypes including up to four processors. Novel techniques are used for task scheduling and garbage collection. The task scheduler helps control excessive resource utilization by means of an unfair scheduling policy: the garbage collector uses a multiprocessor algorithm modeled after the incremental garbage collector of Baker [2]. A companion paper [9] discusses language design issues relating to Multilisp.},
	urldate = {2022-03-30},
	booktitle = {Proceedings of the 1984 {ACM} {Symposium} on {LISP} and functional programming},
	publisher = {Association for Computing Machinery},
	author = {Halstead, Robert H.},
	month = aug,
	year = {1984},
	pages = {9--17},
	file = {Full Text PDF:/Users/neilweidinger/Zotero/storage/YI6FFULM/Halstead - 1984 - Implementation of multilisp Lisp on a multiproces.pdf:application/pdf},
}

@article{halstead_multilisp_1985,
	title = {{MULTILISP}: a language for concurrent symbolic computation},
	volume = {7},
	issn = {0164-0925},
	shorttitle = {{MULTILISP}},
	url = {https://doi.org/10.1145/4472.4478},
	doi = {10.1145/4472.4478},
	abstract = {Multilisp is a version of the Lisp dialect Scheme extended with constructs for parallel execution. Like Scheme, Multilisp is oriented toward symbolic computation. Unlike some parallel programming languages, Multilisp incorporates constructs for causing side effects and for explicitly introducing parallelism. The potential complexity of dealing with side effects in a parallel context is mitigated by the nature of the parallelism constructs and by support for abstract data types: a recommended Multilisp programming style is presented which, if followed, should lead to highly parallel, easily understandable programs. Multilisp is being implemented on the 32-processor Concert multiprocessor; however, it is ultimately intended for use on larger multiprocessors. The current implementation, called Concert Multilisp, is complete enough to run the Multilisp compiler itself and has been run on Concert prototypes including up to eight processors. Concert Multilisp uses novel techniques for task scheduling and garbage collection. The task scheduler helps control excessive resource utilization by means of an unfair scheduling policy; the garbage collector uses a multiprocessor algorithm based on the incremental garbage collector of Baker.},
	number = {4},
	urldate = {2022-03-30},
	journal = {ACM Transactions on Programming Languages and Systems},
	author = {Halstead, Robert H.},
	month = oct,
	year = {1985},
	pages = {501--538},
	file = {Full Text PDF:/Users/neilweidinger/Zotero/storage/WENP2U5G/Halstead - 1985 - MULTILISP a language for concurrent symbolic comp.pdf:application/pdf},
}


@inproceedings{spoonhower_beyond_2009,
	address = {Calgary, AB, Canada},
	title = {Beyond nested parallelism: tight bounds on work-stealing overheads for parallel futures},
	isbn = {978-1-60558-606-9},
	shorttitle = {Beyond nested parallelism},
	url = {http://portal.acm.org/citation.cfm?doid=1583991.1584019},
	doi = {10.1145/1583991.1584019},
	language = {en},
	urldate = {2022-03-30},
	booktitle = {Proceedings of the twenty-first annual symposium on {Parallelism} in algorithms and architectures - {SPAA} '09},
	publisher = {ACM Press},
	author = {Spoonhower, Daniel and Blelloch, Guy E. and Gibbons, Phillip B. and Harper, Robert},
	year = {2009},
	pages = {91},
}

    
@incollection{singer_scheduling_2019,
	series = {Proceedings},
	title = {Scheduling {I}/{O} {Latency}-{Hiding} {Futures} in {Task}-{Parallel} {Platforms}},
	url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611976021.11},
	abstract = {A new method to construct task graphs for \${\textbackslash}mathcal\{H\}\$-matrix arithmetic is introduced, which uses the information associated with all tasks of the standard recursive \${\textbackslash}mathcal\{H\}\$-matrix algorithms, e.g., the block index set of the matrix blocks involved in the computation. Task refinement, i.e., the replacement of tasks by subcomputations, is then used to proceed in the \${\textbackslash}mathcal\{H\}\$-matrix hierarchy until the matrix blocks containing the actual matrix data are reached. This process is a natural extension of the classical, recursive way in which \${\textbackslash}mathcal\{H\}\$-matrix arithmetic is defined and thereby simplifies the efficient usage of many-core systems. Numerical examples for model problems with different block structures demonstrate the various properties of the new approach.},
	urldate = {2022-03-30},
	booktitle = {Symposium on {Algorithmic} {Principles} of {Computer} {Systems} ({APOCS})},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Singer, Kyle and Agrawal, Kunal and Lee, I-Ting Angelina},
	month = dec,
	year = {2019},
	doi = {10.1137/1.9781611976021.11},
	pages = {147--161},
	file = {Full Text:/Users/neilweidinger/Zotero/storage/XEXP8246/Singer et al. - 2019 - Scheduling IO Latency-Hiding Futures in Task-Para.pdf:application/pdf},
}

    
@article{matsakis_rust_2014,
	title = {The rust language},
	volume = {34},
	issn = {1094-3641},
	url = {https://doi.org/10.1145/2692956.2663188},
	doi = {10.1145/2692956.2663188},
	abstract = {Rust is a new programming language for developing reliable and efficient systems. It is designed to support concurrency and parallelism in building applications and libraries that take full advantage of modern hardware. Rust's static type system is safe1 and expressive and provides strong guarantees about isolation, concurrency, and memory safety. Rust also offers a clear performance model, making it easier to predict and reason about program efficiency. One important way it accomplishes this is by allowing fine-grained control over memory representations, with direct support for stack allocation and contiguous record storage. The language balances such controls with the absolute requirement for safety: Rust's type system and runtime guarantee the absence of data races, buffer overflows, stack overflows, and accesses to uninitialized or deallocated memory.},
	number = {3},
	urldate = {2022-03-30},
	journal = {ACM SIGAda Ada Letters},
	author = {Matsakis, Nicholas D. and Klock, Felix S.},
	month = oct,
	year = {2014},
	keywords = {affine type systems, memory management, rust, systems programming},
	pages = {103--104},
}


@misc{noauthor_baby_nodate,
	title = {Baby {Steps}},
	url = {https://smallcultfollowing.com/babysteps/blog/2015/12/18/rayon-data-parallelism-in-rust/},
	urldate = {2022-03-30},
	file = {Baby Steps:/Users/neilweidinger/Zotero/storage/E7Z2MYLS/rayon-data-parallelism-in-rust.html:text/html},
}

@misc{stone_how_2021,
	title = {How {Rust} makes {Rayon}'s data parallelism magical},
	url = {https://developers.redhat.com/blog/2021/04/30/how-rust-makes-rayons-data-parallelism-magical},
	abstract = {The Rayon data parallelism library makes it easy to run your code in parallel—but the real magic comes from tools in the Rust programming language.},
	language = {en},
	urldate = {2022-03-30},
	journal = {Red Hat Developer},
	author = {Stone, Josh},
	month = apr,
	year = {2021},
	file = {Snapshot:/Users/neilweidinger/Zotero/storage/ENUCQD4S/how-rust-makes-rayons-data-parallelism-magical.html:text/html},
}

    
@misc{noauthor_futuresfuture_nodate,
	title = {futures::future - {Rust}},
	url = {https://docs.rs/futures/latest/futures/future/index.html},
	urldate = {2022-03-30},
}

    
@misc{ltd_raspberry_nodate,
	title = {Raspberry {Pi} {Zero} 2 {W}},
	url = {https://www.raspberrypi.com/products/raspberry-pi-zero-2-w/},
	abstract = {Your tiny, tiny \$15 computer},
	language = {en-GB},
	urldate = {2022-04-04},
	journal = {Raspberry Pi},
	author = {Ltd, Raspberry Pi},
}

@misc{noauthor_join_nodate,
	title = {join in futures::future - {Rust}},
	url = {https://docs.rs/futures/latest/futures/future/fn.join.html},
	urldate = {2022-04-04},
	file = {join in futures\:\:future - Rust:/Users/neilweidinger/Zotero/storage/8LSYPSDN/fn.join.html:text/html},
}

@inproceedings{zakian_concurrent_2016,
	address = {Cham},
	title = {Concurrent {Cilk}: {Lazy} {Promotion} from {Tasks} to {Threads} in {C}/{C}++},
	isbn = {978-3-319-29778-1},
	shorttitle = {Concurrent {Cilk}},
	doi = {10.1007/978-3-319-29778-1_5},
	abstract = {Library and language support for scheduling non-blocking tasks has greatly improved, as have lightweight (user) threading packages. However, there is a significant gap between the two developments. In previous work—and in today’s software packages—lightweight thread creation incurs much larger overheads than tasking libraries, even on tasks that end up never blocking. This limitation can be removed. To that end, we describe an extension to the Intel Cilk Plus runtime system, Concurrent Cilk, where tasks are lazily promoted to threads. Concurrent Cilk removes the overhead of thread creation on threads which end up calling no blocking operations, and is the first system to do so for C/C++ with legacy support (standard calling conventions and stack representations). We demonstrate that Concurrent Cilk adds negligible overhead to existing Cilk programs, while its promoted threads remain more efficient than OS threads in terms of context-switch overhead and blocking communication. Further, it enables development of blocking data structures that create non-fork-join dependence graphs—which can expose more parallelism, and better supports data-driven computations waiting on results from remote devices.},
	language = {en},
	booktitle = {Languages and {Compilers} for {Parallel} {Computing}},
	publisher = {Springer International Publishing},
	author = {Zakian, Christopher S. and Zakian, Timothy A. K. and Kulkarni, Abhishek and Chamith, Buddhika and Newton, Ryan R.},
	editor = {Shen, Xipeng and Mueller, Frank and Tuck, James},
	year = {2016},
	pages = {73--90},
	file = {Springer Full Text PDF:/Users/neilweidinger/Zotero/storage/WXRI6S9S/Zakian et al. - 2016 - Concurrent Cilk Lazy Promotion from Tasks to Thre.pdf:application/pdf},
}
